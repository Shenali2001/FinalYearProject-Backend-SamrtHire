{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T09:36:29.145807Z",
     "start_time": "2025-08-02T09:36:23.678977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Step 1: Install Dependencies\n",
    "# Run in a Jupyter cell if not already installed:\n",
    "!pip install transformers datasets evaluate scikit-learn torch numpy"
   ],
   "id": "a57bda5d405408b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Requirement already satisfied: transformers in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (4.53.3)\r\n",
      "Requirement already satisfied: datasets in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (4.0.0)\r\n",
      "Requirement already satisfied: evaluate in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (0.4.5)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (1.7.1)\r\n",
      "Requirement already satisfied: torch in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (2.7.1)\r\n",
      "Requirement already satisfied: numpy in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (2.2.6)\r\n",
      "Requirement already satisfied: filelock in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (3.18.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (0.34.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (2.32.4)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (0.21.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from datasets) (21.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from datasets) (2.3.1)\r\n",
      "Requirement already satisfied: xxhash in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from datasets) (2025.3.0)\r\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from torch) (4.14.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from requests->transformers) (2025.7.14)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T09:36:33.737721Z",
     "start_time": "2025-08-02T09:36:33.717806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('converted_dataset_clean.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "print(f\"Total entries parsed: {len(data)}\")\n",
    "\n",
    "# Separate generation (CV section -> question) and classification (question-answer -> label) data\n",
    "generation_data = [entry for entry in data if \"output\" in entry and \"label\" not in entry]\n",
    "classification_data = [entry for entry in data if \"label\" in entry]\n",
    "\n",
    "# Add technical examples to classification data\n",
    "new_examples = [\n",
    "    {\"input\": \"Question: What was a key collaboration challenge you faced, and how did you address it?\\nAnswer: I used Docker to streamline team workflows.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: How do you integrate backend APIs with front-end?\\nAnswer: I used Spring Boot with React for integration.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: How did you improve debugging efficiency?\\nAnswer: I used automated testing with Selenium.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: What was a key collaboration challenge you faced, and how did you address it?\\nAnswer: I ignored team issues.\", \"label\": \"incorrect\"},\n",
    "    {\"input\": \"Question: How do you integrate backend APIs with front-end?\\nAnswer: I avoided API integration.\", \"label\": \"incorrect\"},\n",
    "    {\"input\": \"Question: How did you improve debugging efficiency?\\nAnswer: I didn’t use debugging tools.\", \"label\": \"incorrect\"},\n",
    "    {\"input\": \"Question: How did you optimize database performance at your job?\\nAnswer: I optimized PostgreSQL queries, reducing response times by 15%.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: How did you optimize database performance at your job?\\nAnswer: I ignored database optimization.\", \"label\": \"incorrect\"},\n",
    "    {\"input\": \"Question: What tools did you use for version control?\\nAnswer: I implemented Git to manage code versions effectively.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: What tools did you use for version control?\\nAnswer: I used manual file tracking instead of version control.\", \"label\": \"incorrect\"},\n",
    "    {\"input\": \"Question: How did you improve application scalability?\\nAnswer: I deployed applications using AWS Elastic Beanstalk.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: How did you improve application scalability?\\nAnswer: I avoided scalability enhancements.\", \"label\": \"incorrect\"},\n",
    "    {\"input\": \"Question: How did you manage financial reporting challenges?\\nAnswer: I automated financial reports using Excel macros.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: How did you manage financial reporting challenges?\\nAnswer: I relied on manual reporting without automation.\", \"label\": \"incorrect\"},\n",
    "    {\"input\": \"Question: What strategies did you use to reduce budget variances?\\nAnswer: I implemented variance analysis with SAP software.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: What strategies did you use to reduce budget variances?\\nAnswer: I ignored budget variance issues.\", \"label\": \"incorrect\"},\n",
    "    {\"input\": \"Question: How did you ensure specimen integrity during collection?\\nAnswer: I followed OSHA guidelines for aseptic techniques.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: How did you ensure specimen integrity during collection?\\nAnswer: I skipped proper collection protocols.\", \"label\": \"incorrect\"},\n",
    "    {\"input\": \"Question: What methods did you use to train new phlebotomists?\\nAnswer: I conducted hands-on training with HIPAA compliance.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: What methods did you use to train new phlebotomists?\\nAnswer: I provided no formal training.\", \"label\": \"incorrect\"},\n",
    "    {\"input\": \"Question: How did you enhance user interface performance?\\nAnswer: I optimized CSS and JavaScript for faster rendering.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: How did you enhance user interface performance?\\nAnswer: I used unoptimized code without improvements.\", \"label\": \"incorrect\"},\n",
    "    {\"input\": \"Question: What approach did you take for cloud migration?\\nAnswer: I used Azure to migrate workloads with zero downtime.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: What approach did you take for cloud migration?\\nAnswer: I avoided cloud migration entirely.\", \"label\": \"incorrect\"}\n",
    "]\n",
    "classification_data.extend(new_examples)\n",
    "\n",
    "# Save updated dataset\n",
    "with open('converted_dataset_clean_updated.json', 'w') as file:\n",
    "    json.dump(classification_data + generation_data, file, indent=2)"
   ],
   "id": "68efdf3a21ae4861",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries parsed: 2541\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T09:19:57.743343Z",
     "start_time": "2025-07-28T09:18:41.411829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "generation_inputs = [\"generate question: \" + entry[\"input\"] for entry in generation_data]\n",
    "generation_targets = [entry[\"output\"] for entry in generation_data]\n",
    "generation_dataset = Dataset.from_dict({\"input_text\": generation_inputs, \"target_text\": generation_targets})\n",
    "\n",
    "tokenizer_gen = T5Tokenizer.from_pretrained('t5-small')\n",
    "def tokenize_generation(examples):\n",
    "    model_inputs = tokenizer_gen(examples[\"input_text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer_gen(examples[\"target_text\"], max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "tokenized_generation = generation_dataset.map(tokenize_generation, batched=True)\n",
    "\n",
    "train_size = int(0.8 * len(tokenized_generation))\n",
    "val_size = len(tokenized_generation) - train_size\n",
    "tokenized_train_gen = tokenized_generation.shuffle(seed=42).select(range(train_size))\n",
    "tokenized_val_gen = tokenized_generation.shuffle(seed=42).select(range(train_size, train_size + val_size))\n",
    "\n",
    "model_gen = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
    "training_args_gen = TrainingArguments(\n",
    "    output_dir='./results_gen',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_gen',\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    dataloader_pin_memory=False\n",
    ")\n",
    "trainer_gen = Trainer(\n",
    "    model=model_gen,\n",
    "    args=training_args_gen,\n",
    "    train_dataset=tokenized_train_gen,\n",
    "    eval_dataset=tokenized_val_gen\n",
    ")\n",
    "trainer_gen.train()\n",
    "model_gen.save_pretrained('./question_generation_model')\n",
    "tokenizer_gen.save_pretrained('./question_generation_model')"
   ],
   "id": "516c2fa43111f801",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /Users/dasunsathsara/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/dasunsathsara/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /Users/dasunsathsara/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/860 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6cc6326e6ee34b4283329901b0d9b4fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/dasunsathsara/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.53.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/dasunsathsara/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /Users/dasunsathsara/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the Training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_text, target_text. If input_text, target_text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 688\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 258\n",
      "  Number of trainable parameters = 60,506,624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='258' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7/258 00:38 < 32:18, 0.13 it/s, Epoch 0.07/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 46\u001B[0m\n\u001B[1;32m     26\u001B[0m training_args_gen \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[1;32m     27\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./results_gen\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     28\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     38\u001B[0m     dataloader_pin_memory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     39\u001B[0m )\n\u001B[1;32m     40\u001B[0m trainer_gen \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     41\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel_gen,\n\u001B[1;32m     42\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args_gen,\n\u001B[1;32m     43\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mtokenized_train_gen,\n\u001B[1;32m     44\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39mtokenized_val_gen\n\u001B[1;32m     45\u001B[0m )\n\u001B[0;32m---> 46\u001B[0m \u001B[43mtrainer_gen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     47\u001B[0m model_gen\u001B[38;5;241m.\u001B[39msave_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./question_generation_model\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     48\u001B[0m tokenizer_gen\u001B[38;5;241m.\u001B[39msave_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./question_generation_model\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/transformers/trainer.py:2206\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2204\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2205\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2206\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2207\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2208\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2209\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2210\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2211\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/transformers/trainer.py:2548\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2541\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2542\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   2543\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2544\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[1;32m   2545\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[1;32m   2546\u001B[0m )\n\u001B[1;32m   2547\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[0;32m-> 2548\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2550\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2551\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2552\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2553\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2554\u001B[0m ):\n\u001B[1;32m   2555\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2556\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/transformers/trainer.py:3797\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m   3794\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m==\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED:\n\u001B[1;32m   3795\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscale_wrt_gas\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m-> 3797\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3799\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach()\n",
      "File \u001B[0;32m~/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:2578\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[0;34m(self, loss, **kwargs)\u001B[0m\n\u001B[1;32m   2576\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n\u001B[1;32m   2577\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2578\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/torch/_tensor.py:648\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    640\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    641\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    646\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    647\u001B[0m     )\n\u001B[0;32m--> 648\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    348\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    350\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 353\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    822\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    823\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 824\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    825\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    826\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    827\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    828\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T09:36:42.421778Z",
     "start_time": "2025-08-02T09:36:42.419653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n"
   ],
   "id": "6d3960e5acdaf949",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T05:07:51.452929Z",
     "start_time": "2025-08-03T05:07:49.077549Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install sentence-transformers",
   "id": "945e91ee08fb47d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\r\n",
      "  Obtaining dependency information for sentence-transformers from https://files.pythonhosted.org/packages/6f/ff/178f08ea5ebc1f9193d9de7f601efe78c01748347875c8438f66f5cecc19/sentence_transformers-5.0.0-py3-none-any.whl.metadata\r\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from sentence-transformers) (4.53.3)\r\n",
      "Requirement already satisfied: tqdm in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from sentence-transformers) (2.7.1)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from sentence-transformers) (1.7.1)\r\n",
      "Requirement already satisfied: scipy in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from sentence-transformers) (0.34.1)\r\n",
      "Requirement already satisfied: Pillow in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from sentence-transformers) (11.3.0)\r\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from sentence-transformers) (4.14.0)\r\n",
      "Requirement already satisfied: filelock in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\r\n",
      "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m470.2/470.2 kB\u001B[0m \u001B[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: sentence-transformers\r\n",
      "Successfully installed sentence-transformers-5.0.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T05:08:21.268315Z",
     "start_time": "2025-08-03T05:08:19.962301Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install transformers datasets torch nltk",
   "id": "1df9cf8257f1a56c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (4.53.3)\r\n",
      "Requirement already satisfied: datasets in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (4.0.0)\r\n",
      "Requirement already satisfied: torch in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (2.7.1)\r\n",
      "Requirement already satisfied: nltk in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (3.9.1)\r\n",
      "Requirement already satisfied: filelock in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (3.18.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (0.34.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (2.2.6)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (2.32.4)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (0.21.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from datasets) (21.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from datasets) (2.3.1)\r\n",
      "Requirement already satisfied: xxhash in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from datasets) (2025.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from torch) (4.14.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: click in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from nltk) (8.2.1)\r\n",
      "Requirement already satisfied: joblib in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from nltk) (1.5.1)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from requests->transformers) (2025.7.14)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T19:07:08.794856Z",
     "start_time": "2025-08-09T18:47:26.766686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import math\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load and deduplicate dataset\n",
    "with open('enhanced_dataset_clean.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "seen = set()\n",
    "unique_data = []\n",
    "for entry in data:\n",
    "    key = (entry.get(\"input\", \"\"), entry.get(\"output\", \"\"))\n",
    "    if key not in seen and \"output\" in entry and \"label\" not in entry and entry[\"output\"].strip():\n",
    "        seen.add(key)\n",
    "        unique_data.append(entry)\n",
    "\n",
    "print(f\"Total unique entries: {len(unique_data)}\")\n",
    "\n",
    "# Enhanced prompt format\n",
    "def create_prompt(entry):\n",
    "    input_text = entry[\"input\"].lower()\n",
    "    if \"education\" in input_text:\n",
    "        difficulty = \"easy\"\n",
    "        q_type = \"education\"\n",
    "    elif any(skill in input_text for skill in [\"skills\", \"python\", \"docker\", \"performance tuning\"]):\n",
    "        difficulty = \"medium\"\n",
    "        q_type = \"skills\"\n",
    "    else:\n",
    "        difficulty = \"hard\"\n",
    "        q_type = \"project\"\n",
    "    return f\"generate question | difficulty={difficulty} | type={q_type} | context={entry['input']} | example=What improved X at Y?\"\n",
    "\n",
    "generation_inputs = [create_prompt(entry) for entry in unique_data]\n",
    "generation_targets = [entry[\"output\"] for entry in unique_data]\n",
    "generation_dataset = Dataset.from_dict({\"input_text\": generation_inputs, \"target_text\": generation_targets})\n",
    "\n",
    "# Tokenization\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(example):\n",
    "    inputs = tokenizer(\n",
    "        example[\"input_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        example[\"target_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=40  # Increased for more context\n",
    "    )\n",
    "    inputs[\"labels\"] = [\n",
    "        (-100 if token == tokenizer.pad_token_id else token) for token in targets[\"input_ids\"]\n",
    "    ]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = generation_dataset.map(tokenize, batched=True, remove_columns=[\"input_text\", \"target_text\"])\n",
    "\n",
    "# Train/val split\n",
    "train_size = int(0.8 * len(tokenized_dataset))\n",
    "val_size = len(tokenized_dataset) - train_size\n",
    "train_dataset = tokenized_dataset.shuffle(seed=42).select(range(train_size))\n",
    "val_dataset = tokenized_dataset.shuffle(seed=42).select(range(train_size, train_size + val_size))\n",
    "\n",
    "# Model with dropout\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "model.config.dropout = 0.1  # Add regularization\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_gen\",\n",
    "    num_train_epochs=20,  # Increased for deeper learning\n",
    "    per_device_train_batch_size=4,  # Increased if hardware allows\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=2e-5,  # Lowered for stability\n",
    "    logging_dir=\"./logs_gen\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,  # Adjusted to be a multiple of eval_steps\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./question_generation_model\")\n",
    "tokenizer.save_pretrained(\"./question_generation_model\")\n",
    "\n",
    "# Enhanced evaluation with post-processing\n",
    "def evaluate_model(model, tokenizer, val_dataset, samples=200):\n",
    "    model.eval()\n",
    "    semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    val_subset = val_dataset.shuffle(seed=42).select(range(min(samples, len(val_dataset))))\n",
    "\n",
    "    total_bleu = 0\n",
    "    total_sem = 0\n",
    "    valid_samples = 0\n",
    "\n",
    "    for sample in val_subset:\n",
    "        input_ids = sample[\"input_ids\"]\n",
    "        input_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "        target_text = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True).replace(\"-100\", \"\").strip()\n",
    "        target_starts_how = target_text.lower().startswith(\"how\")\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_length=40, num_beams=6, temperature=0.7)  # Enhanced generation\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Post-process to match target structure\n",
    "        if target_starts_how and not generated_text.lower().startswith(\"how\"):\n",
    "            generated_text = \"How \" + generated_text.split(\" \", 1)[1] if \" \" in generated_text else \"How \" + generated_text\n",
    "\n",
    "        reference = [target_text.split()]\n",
    "        candidate = generated_text.split()\n",
    "        bleu = sentence_bleu(reference, candidate) if candidate and reference[0] else 0\n",
    "\n",
    "        emb1 = semantic_model.encode(target_text, convert_to_tensor=True)\n",
    "        emb2 = semantic_model.encode(generated_text, convert_to_tensor=True)\n",
    "        semantic = util.pytorch_cos_sim(emb1, emb2).item()\n",
    "\n",
    "        if not math.isnan(bleu) and not math.isnan(semantic) and bleu > 0 and semantic > 0:\n",
    "            total_bleu += bleu\n",
    "            total_sem += semantic\n",
    "            valid_samples += 1\n",
    "\n",
    "            print(f\"Input: {input_text}\\nTarget: {target_text}\\nGenerated: {generated_text}\\nBLEU: {bleu:.4f}, Semantic: {semantic:.4f}\\n\")\n",
    "\n",
    "    if valid_samples > 0:\n",
    "        avg_bleu = total_bleu / valid_samples\n",
    "        avg_sem = total_sem / valid_samples\n",
    "        print(f\"\\nAvg BLEU: {avg_bleu:.4f}, Avg Semantic Similarity: {avg_sem:.4f}\")\n",
    "    else:\n",
    "        print(\"No valid samples to evaluate.\")\n",
    "        avg_bleu, avg_sem = 0, 0\n",
    "\n",
    "    return avg_bleu, avg_sem\n",
    "\n",
    "# Run Evaluation\n",
    "evaluate_model(model, tokenizer, val_dataset)"
   ],
   "id": "967706540cca6d98",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1007)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Total unique entries: 428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/428 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8990abd3e6534827970e76f3bbe5c558"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1720' max='1720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1720/1720 18:27, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.035427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.037838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.037892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: AI Developer, SmartMind Pro, 2020-2022, Enhanced algorithm efficiency by 12%. | example=What improved X at Y?\n",
      "Target: What enhanced efficiency at SmartMind Pro?\n",
      "Generated: What enhanced algorithm efficiency at SmartMind Pro?\n",
      "BLEU: 0.4889, Semantic: 0.9254\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Mosaic Maker, TileArt Pro, 2020-2022, Increased sales by 18%. | example=What improved X at Y?\n",
      "Target: What increased sales at TileArt Pro?\n",
      "Generated: What increased sales at TileArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Fashion Designer, StyleTrendz, 2020-2022, Launched 5 collections. | example=What improved X at Y?\n",
      "Target: How did you launch collections at StyleTrendz?\n",
      "Generated: How did you launch collections at StyleTrendz?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Event Coordinator, CelebrateNow Inc., 2020-2022, Increased attendance by 30% with promotions. | example=What improved X at Y?\n",
      "Target: What promotions increased attendance at CelebrateNow Inc.?\n",
      "Generated: What promotions increased attendance at CelebrateNow Inc.?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Stonemason, RockSolid Pro, 2020-2022, Reduced time by 12%. | example=What improved X at Y?\n",
      "Target: How did you reduce time at RockSolid Pro?\n",
      "Generated: How did you reduce time at RockSolid Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Clinical Researcher, HealthQuest Pro, 2023-2025, Speeded up trials by 20%. | example=What improved X at Y?\n",
      "Target: How did you speed up trials at HealthQuest Pro?\n",
      "Generated: How speeded trials at HealthQuest Pro?\n",
      "BLEU: 0.3259, Semantic: 0.9181\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Sales Associate, RetailPeak Pro, 2022-2024, Raised sales by 10% with promotions. | example=What improved X at Y?\n",
      "Target: What promotions raised sales at RetailPeak Pro?\n",
      "Generated: What promotions raised sales at RetailPeak Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Doll Maker, TinyArt Pro, 2023-2025, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at TinyArt Pro?\n",
      "Generated: What grew sales at TinyArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Astronomer, StarGaze Pro, 2022-2024, Discovered 5 new objects. | example=What improved X at Y?\n",
      "Target: What aided discoveries at StarGaze Pro?\n",
      "Generated: How did you discover objects at StarGaze Pro?\n",
      "BLEU: 0.0000, Semantic: 0.7271\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Wood Turner, SpinArt Pro, 2022-2024, Increased sales by 10%. | example=What improved X at Y?\n",
      "Target: What increased sales at SpinArt Pro?\n",
      "Generated: What increased sales at SpinArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Fabric Dyer, DyeArt Pro, 2022-2024, Boosted sales by 10%. | example=What improved X at Y?\n",
      "Target: What boosted sales at DyeArt Pro?\n",
      "Generated: What boosted sales at DyeArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Leather Carver, HideCraft Pro, 2022-2024, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at HideCraft Pro?\n",
      "Generated: What grew sales at HideCraft Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Tattoo Artist, InkMaster Pro, 2020-2022, Grew clients by 20%. | example=What improved X at Y?\n",
      "Target: What grew clients at InkMaster Pro?\n",
      "Generated: What grew clients at InkMaster Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Glass Carver, ClearCut Pro, 2022-2024, Increased sales by 12%. | example=What improved X at Y?\n",
      "Target: What increased sales at ClearCut Pro?\n",
      "Generated: What increased sales at ClearCut Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Customer Support Specialist, HelpVista Inc., 2021-2023, Improved resolution rate to 90% within 24 hours. | example=What improved X at Y?\n",
      "Target: What improved the resolution rate at HelpVista Inc.?\n",
      "Generated: What improved resolution rate at HelpVista Inc.?\n",
      "BLEU: 0.6130, Semantic: 0.9918\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Animator, MotionMagic Inc., 2019-2021, Created 20 animations. | example=What improved X at Y?\n",
      "Target: How did you create animations at MotionMagic Inc.?\n",
      "Generated: How did you create animations at MotionMagic Inc.?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Toy Designer, PlayTime Pro, 2020-2022, Increased sales by 20%. | example=What improved X at Y?\n",
      "Target: What increased sales at PlayTime Pro?\n",
      "Generated: What increased sales at PlayTime Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Customer Support Lead, HelpCore Pro, 2020-2022, Reduced wait times by 18%. | example=What improved X at Y?\n",
      "Target: What reduced wait times at HelpCore Pro?\n",
      "Generated: How did you reduce wait times at HelpCore Pro?\n",
      "BLEU: 0.4463, Semantic: 0.9458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Customer Service Manager, HelpLink Pro, 2020-2022, Raised satisfaction scores by 15%. | example=What improved X at Y?\n",
      "Target: How did you raise satisfaction at HelpLink Pro?\n",
      "Generated: How did you raise satisfaction scores at HelpLink Pro?\n",
      "BLEU: 0.5969, Semantic: 0.9275\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Historian, TimeTrace Pro, 2022-2024, Enhanced attendance by 25%. | example=What improved X at Y?\n",
      "Target: What enhanced attendance at TimeTrace Pro?\n",
      "Generated: What enhanced attendance at TimeTrace Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Mosaic Maker, TileArt Pro, 2023-2025, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at TileArt Pro?\n",
      "Generated: What grew sales at TileArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Potter, ClayArt Pro, 2022-2024, Grew sales by 20%. | example=What improved X at Y?\n",
      "Target: What grew sales at ClayArt Pro?\n",
      "Generated: What grew sales at ClayArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Event Planner, CelebratePro, 2021-2023, Increased attendance by 40%. | example=What improved X at Y?\n",
      "Target: What increased attendance at CelebratePro?\n",
      "Generated: What increased attendance at CelebratePro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Animator, FrameMaster Pro, 2022-2024, Increased wins by 20%. | example=What improved X at Y?\n",
      "Target: What increased wins at FrameMaster Pro?\n",
      "Generated: What increased wins at FrameMaster Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Electrical Engineer, PowerGrid Solutions, 2021-2023, Improved grid stability by 12%. | example=What improved X at Y?\n",
      "Target: What improved grid stability at PowerGrid Solutions?\n",
      "Generated: How did you improve stability at PowerGrid Solutions?\n",
      "BLEU: 0.3457, Semantic: 0.9133\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Acoustics Engineer, SoundWave Pro, 2022-2024, Reduced noise by 10%. | example=What improved X at Y?\n",
      "Target: What reduced noise at SoundWave Pro?\n",
      "Generated: How did you reduce noise at SoundWave Pro?\n",
      "BLEU: 0.3457, Semantic: 0.9287\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Glass Painter, ColorCraft Pro, 2022-2024, Increased sales by 12%. | example=What improved X at Y?\n",
      "Target: What increased sales at ColorCraft Pro?\n",
      "Generated: What increased sales at ColorCraft Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Speech Therapist, VoiceCare Pro, 2023-2025, Improved clarity by 12%. | example=What improved X at Y?\n",
      "Target: How did you improve clarity at VoiceCare Pro?\n",
      "Generated: How improved clarity at VoiceCare Pro?\n",
      "BLEU: 0.3850, Semantic: 0.9553\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Embedded Developer, CircuitEdge Tech, 2021-2023, Reduced power usage by 15% with firmware. | example=What improved X at Y?\n",
      "Target: What firmware reduced power usage at CircuitEdge Tech?\n",
      "Generated: How did firmware reduce power usage at CircuitEdge Tech?\n",
      "BLEU: 0.4671, Semantic: 0.9464\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Puppet Maker, ToyCraft Pro, 2021-2023, Increased production by 15%. | example=What improved X at Y?\n",
      "Target: What increased production at ToyCraft Pro?\n",
      "Generated: What increased production at ToyCraft Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Landscape Architect, GreenScape Pro, 2021-2023, Reduced water use by 15%. | example=What improved X at Y?\n",
      "Target: How did you reduce water use at GreenScape Pro?\n",
      "Generated: How did you reduce water use at GreenScape Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Legal Advisor, LawFirm Pro, 2020-2022, Won 90% of cases. | example=What improved X at Y?\n",
      "Target: What won cases at LawFirm Pro?\n",
      "Generated: How did you win cases at LawFirm Pro?\n",
      "BLEU: 0.3457, Semantic: 0.9070\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Doll Maker, TinyTreasures Pro, 2021-2023, Boosted sales by 18%. | example=What improved X at Y?\n",
      "Target: What boosted sales at TinyTreasures Pro?\n",
      "Generated: What boosted sales at TinyTreasures Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Software Architect, CodeCraft Pro, 2022-2024, Reduced technical debt by 20%. | example=What improved X at Y?\n",
      "Target: What reduced technical debt at CodeCraft Pro?\n",
      "Generated: How did you reduce technical debt at CodeCraft Pro?\n",
      "BLEU: 0.4463, Semantic: 0.9572\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Basket Weaver, WeaveMaster Pro, 2020-2022, Increased sales by 15%. | example=What improved X at Y?\n",
      "Target: What increased sales at WeaveMaster Pro?\n",
      "Generated: What increased sales at WeaveMaster Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Safety Coordinator, SafeWork Pro, 2022-2024, Decreased incidents by 18% with training. | example=What improved X at Y?\n",
      "Target: How did training decrease incidents at SafeWork Pro?\n",
      "Generated: How did training reduce incidents at SafeWork Pro?\n",
      "BLEU: 0.5000, Semantic: 0.9844\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Bead Weaver, GemBead Pro, 2020-2022, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at GemBead Pro?\n",
      "Generated: What grew sales at GemBead Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Mathematician, NumSolve Labs, 2019-2021, Solved 10 complex problems. | example=What improved X at Y?\n",
      "Target: How did you solve problems at NumSolve Labs?\n",
      "Generated: How did you solve complex problems at NumSolve Labs?\n",
      "BLEU: 0.5969, Semantic: 0.9164\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Warehouse Supervisor, StockFlow Inc., 2019-2021, Increased efficiency by 30%. | example=What improved X at Y?\n",
      "Target: What increased efficiency at StockFlow Inc.?\n",
      "Generated: What increased efficiency at StockFlow Inc.?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Software Engineer, CodeForge Inc., 2022-2024, Reduced bug rates by 25% with code reviews. | example=What improved X at Y?\n",
      "Target: What code reviews reduced bug rates at CodeForge Inc.?\n",
      "Generated: How did code reviews reduce bug rates at CodeForge Inc.?\n",
      "BLEU: 0.4518, Semantic: 0.9417\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Glass Carver, ClearCut Pro, 2020-2022, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at ClearCut Pro?\n",
      "Generated: What grew sales at ClearCut Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Rug Maker, WeftCraft Pro, 2021-2023, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at WeftCraft Pro?\n",
      "Generated: What grew sales at WeftCraft Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Mobile Developer, AppRise Tech, 2020-2022, Increased app downloads by 20% with features. | example=What improved X at Y?\n",
      "Target: What features increased downloads at AppRise Tech?\n",
      "Generated: What features increased downloads at AppRise Tech?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Systems Analyst, TechFlow Innovations, 2023-2025, Enhanced system uptime to 99.5%. | example=What improved X at Y?\n",
      "Target: How did you enhance uptime at TechFlow Innovations?\n",
      "Generated: How enhanced system uptime at TechFlow Innovations?\n",
      "BLEU: 0.3768, Semantic: 0.8480\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Operations Analyst, StreamlinePro Inc., 2022-2024, Reduced process delays by 20% with optimization. | example=What improved X at Y?\n",
      "Target: How did optimization reduce delays at StreamlinePro Inc.?\n",
      "Generated: How did optimization reduce delays at StreamlinePro Inc.?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Metal Engraver, SteelMark Pro, 2021-2023, Grew orders by 10%. | example=What improved X at Y?\n",
      "Target: What grew orders at SteelMark Pro?\n",
      "Generated: What grew orders at SteelMark Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Quilt Maker, PatchWork Pro, 2022-2024, Increased sales by 10%. | example=What improved X at Y?\n",
      "Target: What increased sales at PatchWork Pro?\n",
      "Generated: What increased sales at PatchWork Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Wildlife Ranger, NatureGuard Pro, 2023-2025, Reduced poaching by 12%. | example=What improved X at Y?\n",
      "Target: How did you reduce poaching at NatureGuard Pro?\n",
      "Generated: How did you reduce poaching at NatureGuard Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Lace Maker, ThreadLace Pro, 2023-2025, Increased sales by 18%. | example=What improved X at Y?\n",
      "Target: What increased sales at ThreadLace Pro?\n",
      "Generated: What increased sales at ThreadLace Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Compliance Manager, ReguSafe Inc., 2020-2022, Achieved 95% audit compliance with policies. | example=What improved X at Y?\n",
      "Target: How did policies ensure compliance at ReguSafe Inc.?\n",
      "Generated: How did you achieve compliance with policies at ReguSafe Inc.?\n",
      "BLEU: 0.0000, Semantic: 0.9433\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Leatherworker, HideCraft Pro, 2022-2024, Increased sales by 18%. | example=What improved X at Y?\n",
      "Target: What increased sales at HideCraft Pro?\n",
      "Generated: What increased sales at HideCraft Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Mosaic Maker, TileArt Pro, 2022-2024, Boosted sales by 20%. | example=What improved X at Y?\n",
      "Target: What boosted sales at TileArt Pro?\n",
      "Generated: What boosted sales at TileArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Tapestry Weaver, ThreadTapestry Pro, 2021-2023, Boosted sales by 10%. | example=What improved X at Y?\n",
      "Target: What boosted sales at ThreadTapestry Pro?\n",
      "Generated: What boosted sales at ThreadTapestry Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Training Coordinator, SkillUp Corp., 2019-2021, Trained 200+ employees. | example=What improved X at Y?\n",
      "Target: How did you train employees at SkillUp Corp.?\n",
      "Generated: How did you train employees at SkillUp Corp.?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Candle Designer, WaxDesign Pro, 2020-2022, Increased sales by 12%. | example=What improved X at Y?\n",
      "Target: What increased sales at WaxDesign Pro?\n",
      "Generated: What increased sales at WaxDesign Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Bead Weaver, GemBead Pro, 2023-2025, Boosted sales by 20%. | example=What improved X at Y?\n",
      "Target: What boosted sales at GemBead Pro?\n",
      "Generated: What boosted sales at GemBead Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Soap Maker, PureGlow Pro, 2023-2025, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at PureGlow Pro?\n",
      "Generated: What grew sales at PureGlow Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Wood Sculptor, TimberSculpt Pro, 2023-2025, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at TimberSculpt Pro?\n",
      "Generated: What grew sales at TimberSculpt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Puppeteer, PuppetPlay Pro, 2020-2022, Boosted attendance by 18%. | example=What improved X at Y?\n",
      "Target: What boosted attendance at PuppetPlay Pro?\n",
      "Generated: What boosted attendance at PuppetPlay Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Urban Planner, CityBuild Pro, 2020-2022, Improved traffic by 15%. | example=What improved X at Y?\n",
      "Target: What improved traffic at CityBuild Pro?\n",
      "Generated: How did you improve traffic at CityBuild Pro?\n",
      "BLEU: 0.3457, Semantic: 0.9566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Game Developer, PlayForge Studio, 2020-2022, Released 2 hit games. | example=What improved X at Y?\n",
      "Target: How did you release games at PlayForge Studio?\n",
      "Generated: How did you release hit games at PlayForge Studio?\n",
      "BLEU: 0.5969, Semantic: 0.9478\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Quilt Maker, PatchWork Pro, 2023-2025, Boosted sales by 15%. | example=What improved X at Y?\n",
      "Target: What boosted sales at PatchWork Pro?\n",
      "Generated: What boosted sales at PatchWork Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Research Scientist, BioPeak Labs, 2021-2023, Accelerated experiments by 20% with tools. | example=What improved X at Y?\n",
      "Target: What tools accelerated experiments at BioPeak Labs?\n",
      "Generated: How did tools accelerate experiments at BioPeak Labs?\n",
      "BLEU: 0.3656, Semantic: 0.9423\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Basket Weaver, WeaveArt Pro, 2023-2025, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at WeaveArt Pro?\n",
      "Generated: What grew sales at WeaveArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Candle Maker, GlowLight Pro, 2020-2022, Grew sales by 20%. | example=What improved X at Y?\n",
      "Target: What grew sales at GlowLight Pro?\n",
      "Generated: What grew sales at GlowLight Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Paper Artist, FoldArt Pro, 2023-2025, Increased sales by 10%. | example=What improved X at Y?\n",
      "Target: What increased sales at FoldArt Pro?\n",
      "Generated: What increased sales at FoldArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Real Estate Agent, HomeQuest Pro, 2022-2024, Increased sales by 25%. | example=What improved X at Y?\n",
      "Target: What increased sales at HomeQuest Pro?\n",
      "Generated: What increased sales at HomeQuest Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Business Development Manager, GrowthPulse Pro, 2022-2024, Secured 10 new contracts. | example=What improved X at Y?\n",
      "Target: What secured contracts at GrowthPulse Pro?\n",
      "Generated: How did you secure contracts at GrowthPulse Pro?\n",
      "BLEU: 0.3457, Semantic: 0.9421\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Toy Maker, PlayCraft Pro, 2020-2022, Increased sales by 18%. | example=What improved X at Y?\n",
      "Target: What increased sales at PlayCraft Pro?\n",
      "Generated: What increased sales at PlayCraft Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Optometrist, ClearVision Center, 2020-2022, Reduced appointment delays by 15%. | example=What improved X at Y?\n",
      "Target: What reduced appointment delays at ClearVision Center?\n",
      "Generated: How did you reduce appointment delays at ClearVision Center?\n",
      "BLEU: 0.4463, Semantic: 0.9595\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Mobile App Developer, AppTrendz, 2020-2022, Grew user base by 20%. | example=What improved X at Y?\n",
      "Target: What grew the user base at AppTrendz?\n",
      "Generated: What grew user base at AppTrendz?\n",
      "BLEU: 0.5115, Semantic: 0.9938\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Glassblower, ClearCraft Pro, 2023-2025, Boosted sales by 18%. | example=What improved X at Y?\n",
      "Target: What boosted sales at ClearCraft Pro?\n",
      "Generated: What boosted sales at ClearCraft Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Archivist, HistoryPreserve Pro, 2021-2023, Improved access by 20%. | example=What improved X at Y?\n",
      "Target: What improved access at HistoryPreserve Pro?\n",
      "Generated: How did you improve access at HistoryPreserve Pro?\n",
      "BLEU: 0.3457, Semantic: 0.9459\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Metal Engraver, SteelArt Pro, 2020-2022, Boosted sales by 10%. | example=What improved X at Y?\n",
      "Target: What boosted sales at SteelArt Pro?\n",
      "Generated: What boosted sales at SteelArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Photographer, LensPro Pro, 2022-2024, Grew clients by 20%. | example=What improved X at Y?\n",
      "Target: What grew clients at LensPro Pro?\n",
      "Generated: What grew clients at LensPro Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Content Creator, MediaMix Pro, 2022-2024, Grew audience by 30% with videos. | example=What improved X at Y?\n",
      "Target: What videos grew the audience at MediaMix Pro?\n",
      "Generated: What videos grew audience at MediaMix Pro?\n",
      "BLEU: 0.5154, Semantic: 0.9937\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Systems Architect, TechBuild Corp., 2020-2022, Enhanced system scalability by 15% with redesigns. | example=What improved X at Y?\n",
      "Target: How did redesigns enhance scalability at TechBuild Corp.?\n",
      "Generated: How redesigns enhanced system scalability at TechBuild Corp.?\n",
      "BLEU: 0.3826, Semantic: 0.9075\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Event Photographer, SnapMoment Pro, 2022-2024, Increased bookings by 25%. | example=What improved X at Y?\n",
      "Target: What increased bookings at SnapMoment Pro?\n",
      "Generated: What increased bookings at SnapMoment Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Digital Strategist, MediaPulse Agency, 2023-2025, Boosted ad conversions by 30% with analytics. | example=What improved X at Y?\n",
      "Target: What analytics boosted conversions at MediaPulse Agency?\n",
      "Generated: What analytics boosted conversions at MediaPulse Agency?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Logistics Manager, FreightMaster Inc., 2021-2023, Reduced shipping costs by $300K annually. | example=What improved X at Y?\n",
      "Target: How did you reduce shipping costs at FreightMaster Inc.?\n",
      "Generated: How did you reduce shipping costs at FreightMaster Inc.?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Transportation Planner, MoveEasy Pro, 2021-2023, Reduced fuel costs by 12%. | example=What improved X at Y?\n",
      "Target: How did you reduce fuel costs at MoveEasy Pro?\n",
      "Generated: How did you reduce costs at MoveEasy Pro?\n",
      "BLEU: 0.6102, Semantic: 0.7977\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Veterinary Assistant, PetCare Pro, 2021-2023, Improved recovery by 10%. | example=What improved X at Y?\n",
      "Target: What improved recovery at PetCare Pro?\n",
      "Generated: What improved recovery at PetCare Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Glass Blower, ClearWind Pro, 2021-2023, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at ClearWind Pro?\n",
      "Generated: What grew sales at ClearWind Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Network Administrator, ConnectPro Systems, 2021-2023, Reduced downtime by 18% with network upgrades. | example=What improved X at Y?\n",
      "Target: How did upgrades reduce downtime at ConnectPro Systems?\n",
      "Generated: How network upgrades reduced downtime at ConnectPro Systems?\n",
      "BLEU: 0.3826, Semantic: 0.9328\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Marine Biologist, OceanWatch Pro, 2022-2024, Protected 10% more species. | example=What improved X at Y?\n",
      "Target: How did you protect species at OceanWatch Pro?\n",
      "Generated: How did you protect species at OceanWatch Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n",
      "Input: generate question | difficulty=hard | type=project | context=CV section: Childcare Worker, LittleSteps Pro, 2022-2024, Improved development by 12%. | example=What improved X at Y?\n",
      "Target: How did you improve development at LittleSteps Pro?\n",
      "Generated: How improved development at LittleSteps Pro?\n",
      "BLEU: 0.3850, Semantic: 0.9547\n",
      "\n",
      "\n",
      "Avg BLEU: 0.8019, Avg Semantic Similarity: 0.9762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8019254150813642, 0.9761822445448055)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T21:01:29.561659Z",
     "start_time": "2025-08-09T19:07:57.627299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device} at 01:54 PM +0530 on Monday, July 28, 2025\")\n",
    "\n",
    "# Load the dataset\n",
    "with open('converted_dataset_clean.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "print(f\"Total entries parsed: {len(data)}\")\n",
    "\n",
    "classification_data = [entry for entry in data if \"label\" in entry]\n",
    "\n",
    "# Augment classification data with balanced examples\n",
    "new_examples = [\n",
    "    {\"input\": \"Question: What was a key collaboration challenge?\\nAnswer: I used Docker.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: How do you integrate APIs?\\nAnswer: I used Spring Boot.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: How did you debug?\\nAnswer: I used Selenium.\", \"label\": \"correct\"},\n",
    "    {\"input\": \"Question: What was a challenge?\\nAnswer: I ignored it.\", \"label\": \"incorrect\"},\n",
    "    {\"input\": \"Question: How do you optimize?\\nAnswer: I avoided it.\", \"label\": \"incorrect\"},\n",
    "    {\"input\": \"Question: What tools did you use?\\nAnswer: I used Git.\", \"label\": \"correct\"},\n",
    "]\n",
    "classification_data.extend(new_examples)\n",
    "with open('converted_dataset_clean_updated.json', 'w') as file:\n",
    "    json.dump(classification_data, file, indent=2)\n",
    "\n",
    "# Prepare classification data\n",
    "classification_inputs = [entry[\"input\"] for entry in classification_data]\n",
    "classification_labels = [1 if entry[\"label\"] == \"correct\" else 0 for entry in classification_data]\n",
    "classification_dataset = Dataset.from_dict({\"text\": classification_inputs, \"label\": classification_labels})\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenizer_cls = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def tokenize_classification(examples):\n",
    "    return tokenizer_cls(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "tokenized_classification = classification_dataset.map(tokenize_classification, batched=True)\n",
    "\n",
    "# Convert to list for sklearn compatibility\n",
    "tokenized_data_list = tokenized_classification.to_list()\n",
    "\n",
    "# Split the dataset\n",
    "train_val_data, test_data = train_test_split(tokenized_data_list, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.25, random_state=42)\n",
    "\n",
    "# Convert back to Dataset objects\n",
    "tokenized_train_cls = Dataset.from_list(train_data)\n",
    "tokenized_val_cls = Dataset.from_list(val_data)\n",
    "tokenized_test_cls = Dataset.from_list(test_data)\n",
    "\n",
    "# Initialize and configure the model\n",
    "model_cls = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, hidden_dropout_prob=0.2).to(device)\n",
    "train_labels = [example[\"label\"] for example in tokenized_train_cls]\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=train_labels)\n",
    "print(f\"Class weights: {class_weights}\")  # Debug class weights\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define custom trainer with weighted loss\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Training arguments\n",
    "training_args_cls = TrainingArguments(\n",
    "    output_dir='./results_cls',\n",
    "    num_train_epochs=15,  # Increased from 10 for better learning\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,  # Adjusted for better convergence\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    dataloader_pin_memory=False\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer_cls = WeightedTrainer(\n",
    "    model=model_cls,\n",
    "    args=training_args_cls,\n",
    "    train_dataset=tokenized_train_cls,\n",
    "    eval_dataset=tokenized_val_cls,\n",
    "    compute_metrics=lambda pred: {\"accuracy\": accuracy_score(pred.label_ids, np.argmax(pred.predictions, axis=1))}\n",
    ")\n",
    "trainer_cls.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model_cls.save_pretrained('./answer_evaluation_model')\n",
    "tokenizer_cls.save_pretrained('./answer_evaluation_model')\n",
    "\n",
    "# Evaluate BERT accuracy on test set\n",
    "trainer_cls = WeightedTrainer(\n",
    "    model=model_cls,\n",
    "    args=training_args_cls,\n",
    "    eval_dataset=tokenized_test_cls,\n",
    "    compute_metrics=lambda pred: {\n",
    "        \"accuracy\": accuracy_score(pred.label_ids, np.argmax(pred.predictions, axis=1)),\n",
    "        \"precision\": precision_score(pred.label_ids, np.argmax(pred.predictions, axis=1)),\n",
    "        \"recall\": recall_score(pred.label_ids, np.argmax(pred.predictions, axis=1)),\n",
    "        \"f1\": f1_score(pred.label_ids, np.argmax(pred.predictions, axis=1))\n",
    "    }\n",
    ")\n",
    "test_results = trainer_cls.evaluate()\n",
    "print(\"BERT Test Set Results:\", test_results)\n",
    "\n",
    "predictions = trainer_cls.predict(tokenized_test_cls)\n",
    "labels = predictions.label_ids\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "print(f\"Accuracy: {accuracy_score(labels, preds):.4f}\")\n",
    "print(f\"Precision: {precision_score(labels, preds):.4f}\")\n",
    "print(f\"Recall: {recall_score(labels, preds):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(labels, preds):.4f}\")"
   ],
   "id": "86a7cd875afa819d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps at 01:54 PM +0530 on Monday, July 28, 2025\n",
      "Total entries parsed: 2541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1686 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdf817b41409429db5c887799106d0bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [0.99312377 1.00697211]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1905' max='1905' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1905/1905 1:52:06, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.020113</td>\n",
       "      <td>0.997033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.207527</td>\n",
       "      <td>0.970326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.122168</td>\n",
       "      <td>0.985163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Test Set Results: {'eval_loss': 0.18265974521636963, 'eval_model_preparation_time': 0.0015, 'eval_accuracy': 0.9704142011834319, 'eval_precision': 0.943502824858757, 'eval_recall': 1.0, 'eval_f1': 0.9709302325581395, 'eval_runtime': 35.5487, 'eval_samples_per_second': 9.508, 'eval_steps_per_second': 1.21}\n",
      "Accuracy: 0.9704\n",
      "Precision: 0.9435\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9709\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T03:31:47.903778Z",
     "start_time": "2025-08-10T03:31:47.847293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "model_dir = './answer_evaluation_model'\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Contents of ./answer_evaluation_model:\", os.listdir(model_dir))\n",
    "print(\"Does pytorch_model.bin exist?\", 'pytorch_model.bin' in os.listdir(model_dir))"
   ],
   "id": "f4b2deb94a0844fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/app\n",
      "Contents of ./answer_evaluation_model: ['model.safetensors', 'tokenizer_config.json', 'special_tokens_map.json', 'config.json', 'vocab.txt']\n",
      "Does pytorch_model.bin exist? False\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T03:53:42.077460Z",
     "start_time": "2025-08-10T03:51:49.206500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "import os\n",
    "import math\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Current working directory: {os.getcwd()} at 09:20 AM +0530 on Sunday, August 10, 2025\")\n",
    "\n",
    "# Load and deduplicate dataset\n",
    "try:\n",
    "    with open('enhanced_dataset_clean.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: enhanced_dataset_clean.json not found in the current directory.\")\n",
    "    exit(1)\n",
    "print(f\"Total entries parsed from enhanced_dataset_clean.json: {len(data)}\")\n",
    "\n",
    "seen = set()\n",
    "unique_data = []\n",
    "for entry in data:\n",
    "    key = (entry.get(\"input\", \"\"), entry.get(\"output\", \"\"), entry.get(\"label\", \"\"))\n",
    "    if key not in seen and (\"output\" in entry or \"label\" in entry):\n",
    "        seen.add(key)\n",
    "        unique_data.append(entry)\n",
    "print(f\"Total unique entries in enhanced_dataset_clean.json: {len(unique_data)}\")\n",
    "\n",
    "# Load classification data from converted_dataset_clean_updated.json or converted_dataset_clean.json\n",
    "classification_data = []\n",
    "try:\n",
    "    with open('converted_dataset_clean_updated.json', 'r') as file:\n",
    "        classification_data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        with open('converted_dataset_clean.json', 'r') as file:\n",
    "            classification_data = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Neither converted_dataset_clean_updated.json nor converted_dataset_clean.json found.\")\n",
    "print(f\"Classification entries parsed: {len(classification_data)}\")\n",
    "\n",
    "# Filter classification data to ensure only entries with \"label\" are included\n",
    "classification_data = [entry for entry in classification_data if \"label\" in entry]\n",
    "print(f\"Filtered classification entries: {len(classification_data)}\")\n",
    "\n",
    "# Prepare generation and classification data\n",
    "generation_data = [entry for entry in unique_data if \"output\" in entry and \"label\" not in entry and entry[\"output\"].strip()]\n",
    "print(f\"Generation entries: {len(generation_data)}\")\n",
    "print(f\"Classification entries: {len(classification_data)}\")\n",
    "print(\"Sample generation entry:\", generation_data[:2] if generation_data else \"No generation entries\")\n",
    "print(\"Sample classification entry:\", classification_data[:2] if classification_data else \"No classification entries\")\n",
    "if classification_data:\n",
    "    print(f\"Class distribution: {{'correct': {sum(1 for e in classification_data if e['label'] == 'correct')}, 'incorrect': {sum(1 for e in classification_data if e['label'] == 'incorrect')}}}\")\n",
    "\n",
    "# Step 1: Evaluate T5 Model (Question Generation)\n",
    "try:\n",
    "    tokenizer_gen = T5Tokenizer.from_pretrained('./question_generation_model')\n",
    "    model_gen = T5ForConditionalGeneration.from_pretrained('./question_generation_model').to(device)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading T5 model: {e}\")\n",
    "    model_gen = None\n",
    "\n",
    "if model_gen and generation_data:\n",
    "    # Enhanced prompt format\n",
    "    def create_prompt(entry):\n",
    "        input_text = entry[\"input\"].lower()\n",
    "        if \"education\" in input_text:\n",
    "            difficulty = \"easy\"\n",
    "            q_type = \"education\"\n",
    "        elif any(skill in input_text for skill in [\"skills\", \"python\", \"docker\", \"performance tuning\"]):\n",
    "            difficulty = \"medium\"\n",
    "            q_type = \"skills\"\n",
    "        else:\n",
    "            difficulty = \"hard\"\n",
    "            q_type = \"project\"\n",
    "        return f\"generate question | difficulty={difficulty} | type={q_type} | context={entry['input']} | example=What improved X at Y?\"\n",
    "\n",
    "    generation_inputs = [create_prompt(entry) for entry in generation_data]\n",
    "    generation_targets = [entry[\"output\"] for entry in generation_data]\n",
    "    generation_dataset = Dataset.from_dict({\"input_text\": generation_inputs, \"target_text\": generation_targets})\n",
    "\n",
    "    def tokenize_generation(examples):\n",
    "        model_inputs = tokenizer_gen(examples[\"input_text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "        labels = tokenizer_gen(examples[\"target_text\"], max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_generation = generation_dataset.map(tokenize_generation, batched=True)\n",
    "    train_size = int(0.8 * len(tokenized_generation))\n",
    "    val_size = len(tokenized_generation) - train_size\n",
    "    tokenized_val_gen = tokenized_generation.shuffle(seed=42).select(range(train_size, train_size + val_size))\n",
    "\n",
    "    def evaluate_t5_accuracy(model, tokenizer, val_dataset, num_samples=200):\n",
    "        model.eval()\n",
    "        semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        total_bleu = 0\n",
    "        total_sem = 0\n",
    "        valid_samples = 0\n",
    "        val_samples = val_dataset.shuffle(seed=42).select(range(min(num_samples, len(val_dataset))))\n",
    "\n",
    "        for sample in val_samples:\n",
    "            input_text = sample[\"input_text\"]\n",
    "            target_text = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True).strip()\n",
    "            target_starts_how = target_text.lower().startswith(\"how\")\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_length=128, num_beams=6, temperature=0.7)\n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "            # Post-process to match target structure\n",
    "            if target_starts_how and not generated_text.lower().startswith(\"how\"):\n",
    "                generated_text = \"How \" + generated_text.split(\" \", 1)[1] if \" \" in generated_text else \"How \" + generated_text\n",
    "\n",
    "            reference = [target_text.split()]\n",
    "            candidate = generated_text.split()\n",
    "            bleu = sentence_bleu(reference, candidate) if candidate and reference[0] else 0\n",
    "\n",
    "            emb1 = semantic_model.encode(target_text, convert_to_tensor=True)\n",
    "            emb2 = semantic_model.encode(generated_text, convert_to_tensor=True)\n",
    "            semantic = util.pytorch_cos_sim(emb1, emb2).item()\n",
    "\n",
    "            if not math.isnan(bleu) and not math.isnan(semantic):\n",
    "                total_bleu += bleu\n",
    "                total_sem += semantic\n",
    "                valid_samples += 1\n",
    "                print(f\"Input: {input_text}\\nTarget: {target_text}\\nGenerated: {generated_text}\\nBLEU: {bleu:.4f}, Semantic: {semantic:.4f}\\n\")\n",
    "\n",
    "        if valid_samples > 0:\n",
    "            avg_bleu = total_bleu / valid_samples\n",
    "            avg_sem = total_sem / valid_samples\n",
    "            print(f\"T5 Evaluation on {valid_samples} samples: Avg BLEU: {avg_bleu:.4f}, Avg Semantic Similarity: {avg_sem:.4f}\")\n",
    "        else:\n",
    "            print(\"No valid samples to evaluate.\")\n",
    "            avg_bleu, avg_sem = 0, 0\n",
    "\n",
    "        return avg_bleu, avg_sem\n",
    "\n",
    "    t5_bleu, t5_sem = evaluate_t5_accuracy(model_gen, tokenizer_gen, tokenized_val_gen)\n",
    "else:\n",
    "    print(\"No generation data or T5 model available. Skipping T5 evaluation.\")\n",
    "    t5_bleu, t5_sem = 0, 0\n",
    "\n",
    "# Step 2: Evaluate BERT Model (Answer Evaluation)\n",
    "model_dir = os.path.abspath('./answer_evaluation_model')\n",
    "print(f\"Checking model directory: {model_dir}\")\n",
    "print(f\"Directory contents: {os.listdir(model_dir)}\")\n",
    "if not os.path.exists(model_dir) or ('pytorch_model.bin' not in os.listdir(model_dir) and 'model.safetensors' not in os.listdir(model_dir)):\n",
    "    print(f\"Error: Model weights (pytorch_model.bin or model.safetensors) not found in {model_dir}. Please re-run the training script.\")\n",
    "else:\n",
    "    try:\n",
    "        tokenizer_cls = BertTokenizer.from_pretrained(model_dir)\n",
    "        model_cls = BertForSequenceClassification.from_pretrained(model_dir, use_safetensors=True).to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading BERT model: {e}\")\n",
    "        model_cls = None\n",
    "    else:\n",
    "        if classification_data:\n",
    "            classification_inputs = [entry[\"input\"] for entry in classification_data]\n",
    "            classification_labels = [1 if entry[\"label\"] == \"correct\" else 0 for entry in classification_data]\n",
    "            classification_dataset = Dataset.from_dict({\"text\": classification_inputs, \"label\": classification_labels})\n",
    "\n",
    "            def tokenize_classification(examples):\n",
    "                return tokenizer_cls(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "            tokenized_classification = classification_dataset.map(tokenize_classification, batched=True)\n",
    "            tokenized_data_list = tokenized_classification.to_list()\n",
    "\n",
    "            if tokenized_data_list:\n",
    "                train_val_data, test_data = train_test_split(tokenized_data_list, test_size=0.2, random_state=42)\n",
    "                tokenized_test_cls = Dataset.from_list(test_data)\n",
    "\n",
    "                class WeightedTrainer(Trainer):\n",
    "                    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "                        labels = inputs.get(\"labels\").to(device)\n",
    "                        outputs = model(**inputs)\n",
    "                        logits = outputs.get(\"logits\")\n",
    "                        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "                        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "                        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "                training_args_cls = TrainingArguments(\n",
    "                    output_dir='./results_cls_eval',\n",
    "                    per_device_eval_batch_size=8,\n",
    "                    dataloader_pin_memory=False\n",
    "                )\n",
    "\n",
    "                trainer_cls = WeightedTrainer(\n",
    "                    model=model_cls,\n",
    "                    args=training_args_cls,\n",
    "                    eval_dataset=tokenized_test_cls,\n",
    "                    compute_metrics=lambda pred: {\n",
    "                        \"accuracy\": accuracy_score(pred.label_ids, np.argmax(pred.predictions, axis=1)),\n",
    "                        \"precision\": precision_score(pred.label_ids, np.argmax(pred.predictions, axis=1), zero_division=0),\n",
    "                        \"recall\": recall_score(pred.label_ids, np.argmax(pred.predictions, axis=1), zero_division=0),\n",
    "                        \"f1\": f1_score(pred.label_ids, np.argmax(pred.predictions, axis=1), zero_division=0)\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Evaluate BERT model\n",
    "                test_results = trainer_cls.evaluate()\n",
    "                print(\"BERT Test Set Results:\", test_results)\n",
    "\n",
    "                # Additional detailed metrics\n",
    "                predictions = trainer_cls.predict(tokenized_test_cls)\n",
    "                labels = predictions.label_ids\n",
    "                preds = np.argmax(predictions.predictions, axis=1)\n",
    "                print(f\"Accuracy: {accuracy_score(labels, preds):.4f}\")\n",
    "                print(f\"Precision: {precision_score(labels, preds, zero_division=0):.4f}\")\n",
    "                print(f\"Recall: {recall_score(labels, preds, zero_division=0):.4f}\")\n",
    "                print(f\"F1 Score: {f1_score(labels, preds, zero_division=0):.4f}\")\n",
    "            else:\n",
    "                print(\"No tokenized classification data available. Skipping BERT evaluation.\")\n",
    "        else:\n",
    "            print(\"No classification data available. Skipping BERT evaluation.\")"
   ],
   "id": "e200e92982255c85",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1007)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Current working directory: /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/app at 09:20 AM +0530 on Sunday, August 10, 2025\n",
      "Total entries parsed from enhanced_dataset_clean.json: 428\n",
      "Total unique entries in enhanced_dataset_clean.json: 428\n",
      "Classification entries parsed: 1686\n",
      "Filtered classification entries: 1686\n",
      "Generation entries: 428\n",
      "Classification entries: 1686\n",
      "Sample generation entry: [{'input': 'CV section: Marketing Analyst, TrendWave Corp., 2023-2025, Increased lead generation by 40% with targeted campaigns.', 'output': 'How did you increase lead generation at TrendWave Corp.?'}, {'input': 'CV section: Software Engineer, CodeForge Inc., 2022-2024, Reduced bug rates by 25% with code reviews.', 'output': 'What code reviews reduced bug rates at CodeForge Inc.?'}]\n",
      "Sample classification entry: [{'input': 'Question: What educational qualifications do you have in accounting?\\nAnswer: I have an Associate in Accounting from Northern Maine Community College with a GPA of 3.41, 78 semester hours toward a Bachelor’s in Accounting from Hudson College, and a Certified Defense Financial Manager certification from 2005.', 'label': 'correct'}, {'input': 'Question: What educational qualifications do you have in accounting?\\nAnswer: I have a Master’s in Accounting from Hudson College and a CPA certification.', 'label': 'incorrect'}]\n",
      "Class distribution: {'correct': 844, 'incorrect': 842}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/428 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7eb3277fc4a54497ab83641d28a886b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: AI Developer, SmartMind Pro, 2020-2022, Enhanced algorithm efficiency by 12%. | example=What improved X at Y?\n",
      "Target: What enhanced efficiency at SmartMind Pro?\n",
      "Generated: What enhanced algorithm efficiency at SmartMind Pro?\n",
      "BLEU: 0.4889, Semantic: 0.9254\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Mosaic Maker, TileArt Pro, 2020-2022, Increased sales by 18%. | example=What improved X at Y?\n",
      "Target: What increased sales at TileArt Pro?\n",
      "Generated: What increased sales at TileArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Fashion Designer, StyleTrendz, 2020-2022, Launched 5 collections. | example=What improved X at Y?\n",
      "Target: How did you launch collections at StyleTrendz?\n",
      "Generated: How did you launch collections at StyleTrendz?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Event Coordinator, CelebrateNow Inc., 2020-2022, Increased attendance by 30% with promotions. | example=What improved X at Y?\n",
      "Target: What promotions increased attendance at CelebrateNow Inc.?\n",
      "Generated: What promotions increased attendance at CelebrateNow Inc.?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Stonemason, RockSolid Pro, 2020-2022, Reduced time by 12%. | example=What improved X at Y?\n",
      "Target: How did you reduce time at RockSolid Pro?\n",
      "Generated: How did you reduce time at RockSolid Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Clinical Researcher, HealthQuest Pro, 2023-2025, Speeded up trials by 20%. | example=What improved X at Y?\n",
      "Target: How did you speed up trials at HealthQuest Pro?\n",
      "Generated: How speeded trials at HealthQuest Pro?\n",
      "BLEU: 0.3259, Semantic: 0.9181\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Sales Associate, RetailPeak Pro, 2022-2024, Raised sales by 10% with promotions. | example=What improved X at Y?\n",
      "Target: What promotions raised sales at RetailPeak Pro?\n",
      "Generated: What promotions raised sales at RetailPeak Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Doll Maker, TinyArt Pro, 2023-2025, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at TinyArt Pro?\n",
      "Generated: What grew sales at TinyArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Astronomer, StarGaze Pro, 2022-2024, Discovered 5 new objects. | example=What improved X at Y?\n",
      "Target: What aided discoveries at StarGaze Pro?\n",
      "Generated: How did you discover objects at StarGaze Pro?\n",
      "BLEU: 0.0000, Semantic: 0.7271\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Wood Turner, SpinArt Pro, 2022-2024, Increased sales by 10%. | example=What improved X at Y?\n",
      "Target: What increased sales at SpinArt Pro?\n",
      "Generated: What increased sales at SpinArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Fabric Dyer, DyeArt Pro, 2022-2024, Boosted sales by 10%. | example=What improved X at Y?\n",
      "Target: What boosted sales at DyeArt Pro?\n",
      "Generated: What boosted sales at DyeArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Leather Carver, HideCraft Pro, 2022-2024, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at HideCraft Pro?\n",
      "Generated: What grew sales at HideCraft Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Tattoo Artist, InkMaster Pro, 2020-2022, Grew clients by 20%. | example=What improved X at Y?\n",
      "Target: What grew clients at InkMaster Pro?\n",
      "Generated: What grew clients at InkMaster Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Glass Carver, ClearCut Pro, 2022-2024, Increased sales by 12%. | example=What improved X at Y?\n",
      "Target: What increased sales at ClearCut Pro?\n",
      "Generated: What increased sales at ClearCut Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Customer Support Specialist, HelpVista Inc., 2021-2023, Improved resolution rate to 90% within 24 hours. | example=What improved X at Y?\n",
      "Target: What improved the resolution rate at HelpVista Inc.?\n",
      "Generated: What improved resolution rate at HelpVista Inc.?\n",
      "BLEU: 0.6130, Semantic: 0.9918\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Animator, MotionMagic Inc., 2019-2021, Created 20 animations. | example=What improved X at Y?\n",
      "Target: How did you create animations at MotionMagic Inc.?\n",
      "Generated: How did you create animations at MotionMagic Inc.?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Toy Designer, PlayTime Pro, 2020-2022, Increased sales by 20%. | example=What improved X at Y?\n",
      "Target: What increased sales at PlayTime Pro?\n",
      "Generated: What increased sales at PlayTime Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Customer Support Lead, HelpCore Pro, 2020-2022, Reduced wait times by 18%. | example=What improved X at Y?\n",
      "Target: What reduced wait times at HelpCore Pro?\n",
      "Generated: How did you reduce wait times at HelpCore Pro?\n",
      "BLEU: 0.4463, Semantic: 0.9458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Customer Service Manager, HelpLink Pro, 2020-2022, Raised satisfaction scores by 15%. | example=What improved X at Y?\n",
      "Target: How did you raise satisfaction at HelpLink Pro?\n",
      "Generated: How did you raise satisfaction scores at HelpLink Pro?\n",
      "BLEU: 0.5969, Semantic: 0.9275\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Historian, TimeTrace Pro, 2022-2024, Enhanced attendance by 25%. | example=What improved X at Y?\n",
      "Target: What enhanced attendance at TimeTrace Pro?\n",
      "Generated: What enhanced attendance at TimeTrace Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Mosaic Maker, TileArt Pro, 2023-2025, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at TileArt Pro?\n",
      "Generated: What grew sales at TileArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Potter, ClayArt Pro, 2022-2024, Grew sales by 20%. | example=What improved X at Y?\n",
      "Target: What grew sales at ClayArt Pro?\n",
      "Generated: What grew sales at ClayArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Event Planner, CelebratePro, 2021-2023, Increased attendance by 40%. | example=What improved X at Y?\n",
      "Target: What increased attendance at CelebratePro?\n",
      "Generated: What increased attendance at CelebratePro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Animator, FrameMaster Pro, 2022-2024, Increased wins by 20%. | example=What improved X at Y?\n",
      "Target: What increased wins at FrameMaster Pro?\n",
      "Generated: What increased wins at FrameMaster Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Electrical Engineer, PowerGrid Solutions, 2021-2023, Improved grid stability by 12%. | example=What improved X at Y?\n",
      "Target: What improved grid stability at PowerGrid Solutions?\n",
      "Generated: How did you improve stability at PowerGrid Solutions?\n",
      "BLEU: 0.3457, Semantic: 0.9133\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Acoustics Engineer, SoundWave Pro, 2022-2024, Reduced noise by 10%. | example=What improved X at Y?\n",
      "Target: What reduced noise at SoundWave Pro?\n",
      "Generated: How did you reduce noise at SoundWave Pro?\n",
      "BLEU: 0.3457, Semantic: 0.9287\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Glass Painter, ColorCraft Pro, 2022-2024, Increased sales by 12%. | example=What improved X at Y?\n",
      "Target: What increased sales at ColorCraft Pro?\n",
      "Generated: What increased sales at ColorCraft Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Speech Therapist, VoiceCare Pro, 2023-2025, Improved clarity by 12%. | example=What improved X at Y?\n",
      "Target: How did you improve clarity at VoiceCare Pro?\n",
      "Generated: How improved clarity at VoiceCare Pro?\n",
      "BLEU: 0.3850, Semantic: 0.9553\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Embedded Developer, CircuitEdge Tech, 2021-2023, Reduced power usage by 15% with firmware. | example=What improved X at Y?\n",
      "Target: What firmware reduced power usage at CircuitEdge Tech?\n",
      "Generated: How did firmware reduce power usage at CircuitEdge Tech?\n",
      "BLEU: 0.4671, Semantic: 0.9464\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Puppet Maker, ToyCraft Pro, 2021-2023, Increased production by 15%. | example=What improved X at Y?\n",
      "Target: What increased production at ToyCraft Pro?\n",
      "Generated: What increased production at ToyCraft Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Landscape Architect, GreenScape Pro, 2021-2023, Reduced water use by 15%. | example=What improved X at Y?\n",
      "Target: How did you reduce water use at GreenScape Pro?\n",
      "Generated: How did you reduce water use at GreenScape Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Legal Advisor, LawFirm Pro, 2020-2022, Won 90% of cases. | example=What improved X at Y?\n",
      "Target: What won cases at LawFirm Pro?\n",
      "Generated: How did you win cases at LawFirm Pro?\n",
      "BLEU: 0.3457, Semantic: 0.9070\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Doll Maker, TinyTreasures Pro, 2021-2023, Boosted sales by 18%. | example=What improved X at Y?\n",
      "Target: What boosted sales at TinyTreasures Pro?\n",
      "Generated: What boosted sales at TinyTreasures Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Software Architect, CodeCraft Pro, 2022-2024, Reduced technical debt by 20%. | example=What improved X at Y?\n",
      "Target: What reduced technical debt at CodeCraft Pro?\n",
      "Generated: How did you reduce technical debt at CodeCraft Pro?\n",
      "BLEU: 0.4463, Semantic: 0.9572\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Basket Weaver, WeaveMaster Pro, 2020-2022, Increased sales by 15%. | example=What improved X at Y?\n",
      "Target: What increased sales at WeaveMaster Pro?\n",
      "Generated: What increased sales at WeaveMaster Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Safety Coordinator, SafeWork Pro, 2022-2024, Decreased incidents by 18% with training. | example=What improved X at Y?\n",
      "Target: How did training decrease incidents at SafeWork Pro?\n",
      "Generated: How did training reduce incidents at SafeWork Pro?\n",
      "BLEU: 0.5000, Semantic: 0.9844\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Bead Weaver, GemBead Pro, 2020-2022, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at GemBead Pro?\n",
      "Generated: What grew sales at GemBead Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Mathematician, NumSolve Labs, 2019-2021, Solved 10 complex problems. | example=What improved X at Y?\n",
      "Target: How did you solve problems at NumSolve Labs?\n",
      "Generated: How did you solve complex problems at NumSolve Labs?\n",
      "BLEU: 0.5969, Semantic: 0.9164\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Warehouse Supervisor, StockFlow Inc., 2019-2021, Increased efficiency by 30%. | example=What improved X at Y?\n",
      "Target: What increased efficiency at StockFlow Inc.?\n",
      "Generated: What increased efficiency at StockFlow Inc.?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Software Engineer, CodeForge Inc., 2022-2024, Reduced bug rates by 25% with code reviews. | example=What improved X at Y?\n",
      "Target: What code reviews reduced bug rates at CodeForge Inc.?\n",
      "Generated: How did code reviews reduce bug rates at CodeForge Inc.?\n",
      "BLEU: 0.4518, Semantic: 0.9417\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Glass Carver, ClearCut Pro, 2020-2022, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at ClearCut Pro?\n",
      "Generated: What grew sales at ClearCut Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Rug Maker, WeftCraft Pro, 2021-2023, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at WeftCraft Pro?\n",
      "Generated: What grew sales at WeftCraft Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Mobile Developer, AppRise Tech, 2020-2022, Increased app downloads by 20% with features. | example=What improved X at Y?\n",
      "Target: What features increased downloads at AppRise Tech?\n",
      "Generated: What features increased downloads at AppRise Tech?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Systems Analyst, TechFlow Innovations, 2023-2025, Enhanced system uptime to 99.5%. | example=What improved X at Y?\n",
      "Target: How did you enhance uptime at TechFlow Innovations?\n",
      "Generated: How enhanced system uptime at TechFlow Innovations?\n",
      "BLEU: 0.3768, Semantic: 0.8480\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Operations Analyst, StreamlinePro Inc., 2022-2024, Reduced process delays by 20% with optimization. | example=What improved X at Y?\n",
      "Target: How did optimization reduce delays at StreamlinePro Inc.?\n",
      "Generated: How did optimization reduce delays at StreamlinePro Inc.?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Metal Engraver, SteelMark Pro, 2021-2023, Grew orders by 10%. | example=What improved X at Y?\n",
      "Target: What grew orders at SteelMark Pro?\n",
      "Generated: What grew orders at SteelMark Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Quilt Maker, PatchWork Pro, 2022-2024, Increased sales by 10%. | example=What improved X at Y?\n",
      "Target: What increased sales at PatchWork Pro?\n",
      "Generated: What increased sales at PatchWork Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Wildlife Ranger, NatureGuard Pro, 2023-2025, Reduced poaching by 12%. | example=What improved X at Y?\n",
      "Target: How did you reduce poaching at NatureGuard Pro?\n",
      "Generated: How did you reduce poaching at NatureGuard Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Lace Maker, ThreadLace Pro, 2023-2025, Increased sales by 18%. | example=What improved X at Y?\n",
      "Target: What increased sales at ThreadLace Pro?\n",
      "Generated: What increased sales at ThreadLace Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Compliance Manager, ReguSafe Inc., 2020-2022, Achieved 95% audit compliance with policies. | example=What improved X at Y?\n",
      "Target: How did policies ensure compliance at ReguSafe Inc.?\n",
      "Generated: How did you achieve compliance with policies at ReguSafe Inc.?\n",
      "BLEU: 0.0000, Semantic: 0.9433\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Leatherworker, HideCraft Pro, 2022-2024, Increased sales by 18%. | example=What improved X at Y?\n",
      "Target: What increased sales at HideCraft Pro?\n",
      "Generated: What increased sales at HideCraft Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Mosaic Maker, TileArt Pro, 2022-2024, Boosted sales by 20%. | example=What improved X at Y?\n",
      "Target: What boosted sales at TileArt Pro?\n",
      "Generated: What boosted sales at TileArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Tapestry Weaver, ThreadTapestry Pro, 2021-2023, Boosted sales by 10%. | example=What improved X at Y?\n",
      "Target: What boosted sales at ThreadTapestry Pro?\n",
      "Generated: What boosted sales at ThreadTapestry Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Training Coordinator, SkillUp Corp., 2019-2021, Trained 200+ employees. | example=What improved X at Y?\n",
      "Target: How did you train employees at SkillUp Corp.?\n",
      "Generated: How did you train employees at SkillUp Corp.?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Candle Designer, WaxDesign Pro, 2020-2022, Increased sales by 12%. | example=What improved X at Y?\n",
      "Target: What increased sales at WaxDesign Pro?\n",
      "Generated: What increased sales at WaxDesign Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Bead Weaver, GemBead Pro, 2023-2025, Boosted sales by 20%. | example=What improved X at Y?\n",
      "Target: What boosted sales at GemBead Pro?\n",
      "Generated: What boosted sales at GemBead Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Soap Maker, PureGlow Pro, 2023-2025, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at PureGlow Pro?\n",
      "Generated: What grew sales at PureGlow Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Wood Sculptor, TimberSculpt Pro, 2023-2025, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at TimberSculpt Pro?\n",
      "Generated: What grew sales at TimberSculpt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Puppeteer, PuppetPlay Pro, 2020-2022, Boosted attendance by 18%. | example=What improved X at Y?\n",
      "Target: What boosted attendance at PuppetPlay Pro?\n",
      "Generated: What boosted attendance at PuppetPlay Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Urban Planner, CityBuild Pro, 2020-2022, Improved traffic by 15%. | example=What improved X at Y?\n",
      "Target: What improved traffic at CityBuild Pro?\n",
      "Generated: How did you improve traffic at CityBuild Pro?\n",
      "BLEU: 0.3457, Semantic: 0.9566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Game Developer, PlayForge Studio, 2020-2022, Released 2 hit games. | example=What improved X at Y?\n",
      "Target: How did you release games at PlayForge Studio?\n",
      "Generated: How did you release hit games at PlayForge Studio?\n",
      "BLEU: 0.5969, Semantic: 0.9478\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Quilt Maker, PatchWork Pro, 2023-2025, Boosted sales by 15%. | example=What improved X at Y?\n",
      "Target: What boosted sales at PatchWork Pro?\n",
      "Generated: What boosted sales at PatchWork Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Research Scientist, BioPeak Labs, 2021-2023, Accelerated experiments by 20% with tools. | example=What improved X at Y?\n",
      "Target: What tools accelerated experiments at BioPeak Labs?\n",
      "Generated: How did tools accelerate experiments at BioPeak Labs?\n",
      "BLEU: 0.3656, Semantic: 0.9423\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Basket Weaver, WeaveArt Pro, 2023-2025, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at WeaveArt Pro?\n",
      "Generated: What grew sales at WeaveArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Candle Maker, GlowLight Pro, 2020-2022, Grew sales by 20%. | example=What improved X at Y?\n",
      "Target: What grew sales at GlowLight Pro?\n",
      "Generated: What grew sales at GlowLight Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Paper Artist, FoldArt Pro, 2023-2025, Increased sales by 10%. | example=What improved X at Y?\n",
      "Target: What increased sales at FoldArt Pro?\n",
      "Generated: What increased sales at FoldArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Real Estate Agent, HomeQuest Pro, 2022-2024, Increased sales by 25%. | example=What improved X at Y?\n",
      "Target: What increased sales at HomeQuest Pro?\n",
      "Generated: What increased sales at HomeQuest Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Business Development Manager, GrowthPulse Pro, 2022-2024, Secured 10 new contracts. | example=What improved X at Y?\n",
      "Target: What secured contracts at GrowthPulse Pro?\n",
      "Generated: How did you secure contracts at GrowthPulse Pro?\n",
      "BLEU: 0.3457, Semantic: 0.9421\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Toy Maker, PlayCraft Pro, 2020-2022, Increased sales by 18%. | example=What improved X at Y?\n",
      "Target: What increased sales at PlayCraft Pro?\n",
      "Generated: What increased sales at PlayCraft Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Optometrist, ClearVision Center, 2020-2022, Reduced appointment delays by 15%. | example=What improved X at Y?\n",
      "Target: What reduced appointment delays at ClearVision Center?\n",
      "Generated: How did you reduce appointment delays at ClearVision Center?\n",
      "BLEU: 0.4463, Semantic: 0.9595\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Mobile App Developer, AppTrendz, 2020-2022, Grew user base by 20%. | example=What improved X at Y?\n",
      "Target: What grew the user base at AppTrendz?\n",
      "Generated: What grew user base at AppTrendz?\n",
      "BLEU: 0.5115, Semantic: 0.9938\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Glassblower, ClearCraft Pro, 2023-2025, Boosted sales by 18%. | example=What improved X at Y?\n",
      "Target: What boosted sales at ClearCraft Pro?\n",
      "Generated: What boosted sales at ClearCraft Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Archivist, HistoryPreserve Pro, 2021-2023, Improved access by 20%. | example=What improved X at Y?\n",
      "Target: What improved access at HistoryPreserve Pro?\n",
      "Generated: How did you improve access at HistoryPreserve Pro?\n",
      "BLEU: 0.3457, Semantic: 0.9459\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Metal Engraver, SteelArt Pro, 2020-2022, Boosted sales by 10%. | example=What improved X at Y?\n",
      "Target: What boosted sales at SteelArt Pro?\n",
      "Generated: What boosted sales at SteelArt Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Photographer, LensPro Pro, 2022-2024, Grew clients by 20%. | example=What improved X at Y?\n",
      "Target: What grew clients at LensPro Pro?\n",
      "Generated: What grew clients at LensPro Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Content Creator, MediaMix Pro, 2022-2024, Grew audience by 30% with videos. | example=What improved X at Y?\n",
      "Target: What videos grew the audience at MediaMix Pro?\n",
      "Generated: What videos grew audience at MediaMix Pro?\n",
      "BLEU: 0.5154, Semantic: 0.9937\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Systems Architect, TechBuild Corp., 2020-2022, Enhanced system scalability by 15% with redesigns. | example=What improved X at Y?\n",
      "Target: How did redesigns enhance scalability at TechBuild Corp.?\n",
      "Generated: How redesigns enhanced system scalability at TechBuild Corp.?\n",
      "BLEU: 0.3826, Semantic: 0.9075\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Event Photographer, SnapMoment Pro, 2022-2024, Increased bookings by 25%. | example=What improved X at Y?\n",
      "Target: What increased bookings at SnapMoment Pro?\n",
      "Generated: What increased bookings at SnapMoment Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Digital Strategist, MediaPulse Agency, 2023-2025, Boosted ad conversions by 30% with analytics. | example=What improved X at Y?\n",
      "Target: What analytics boosted conversions at MediaPulse Agency?\n",
      "Generated: What analytics boosted conversions at MediaPulse Agency?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Logistics Manager, FreightMaster Inc., 2021-2023, Reduced shipping costs by $300K annually. | example=What improved X at Y?\n",
      "Target: How did you reduce shipping costs at FreightMaster Inc.?\n",
      "Generated: How did you reduce shipping costs at FreightMaster Inc.?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Transportation Planner, MoveEasy Pro, 2021-2023, Reduced fuel costs by 12%. | example=What improved X at Y?\n",
      "Target: How did you reduce fuel costs at MoveEasy Pro?\n",
      "Generated: How did you reduce costs at MoveEasy Pro?\n",
      "BLEU: 0.6102, Semantic: 0.7977\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Veterinary Assistant, PetCare Pro, 2021-2023, Improved recovery by 10%. | example=What improved X at Y?\n",
      "Target: What improved recovery at PetCare Pro?\n",
      "Generated: What improved recovery at PetCare Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Glass Blower, ClearWind Pro, 2021-2023, Grew sales by 15%. | example=What improved X at Y?\n",
      "Target: What grew sales at ClearWind Pro?\n",
      "Generated: What grew sales at ClearWind Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Network Administrator, ConnectPro Systems, 2021-2023, Reduced downtime by 18% with network upgrades. | example=What improved X at Y?\n",
      "Target: How did upgrades reduce downtime at ConnectPro Systems?\n",
      "Generated: How network upgrades reduced downtime at ConnectPro Systems?\n",
      "BLEU: 0.3826, Semantic: 0.9328\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: generate question | difficulty=hard | type=project | context=CV section: Marine Biologist, OceanWatch Pro, 2022-2024, Protected 10% more species. | example=What improved X at Y?\n",
      "Target: How did you protect species at OceanWatch Pro?\n",
      "Generated: How did you protect species at OceanWatch Pro?\n",
      "BLEU: 1.0000, Semantic: 1.0000\n",
      "\n",
      "Input: generate question | difficulty=hard | type=project | context=CV section: Childcare Worker, LittleSteps Pro, 2022-2024, Improved development by 12%. | example=What improved X at Y?\n",
      "Target: How did you improve development at LittleSteps Pro?\n",
      "Generated: How improved development at LittleSteps Pro?\n",
      "BLEU: 0.3850, Semantic: 0.9547\n",
      "\n",
      "T5 Evaluation on 86 samples: Avg BLEU: 0.8019, Avg Semantic Similarity: 0.9762\n",
      "Checking model directory: /Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/app/answer_evaluation_model\n",
      "Directory contents: ['model.safetensors', 'tokenizer_config.json', 'special_tokens_map.json', 'config.json', 'vocab.txt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1686 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d85dafd9484b47759607f6bc89415ca4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Test Set Results: {'eval_loss': 0.18401388823986053, 'eval_model_preparation_time': 0.0014, 'eval_accuracy': 0.9704142011834319, 'eval_precision': 0.943502824858757, 'eval_recall': 1.0, 'eval_f1': 0.9709302325581395, 'eval_runtime': 27.0463, 'eval_samples_per_second': 12.497, 'eval_steps_per_second': 1.59}\n",
      "Accuracy: 0.9704\n",
      "Precision: 0.9435\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9709\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T05:03:05.489215Z",
     "start_time": "2025-08-26T04:55:28.026282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import math\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments,  # <-- use seq2seq classes\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ---------------------------\n",
    "# Device\n",
    "# ---------------------------\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(f\"[QG] Using device: {device}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Load + dedupe data\n",
    "# ---------------------------\n",
    "with open(\"enhanced_dataset_clean.json\", \"r\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "seen = set()\n",
    "examples = []\n",
    "for e in raw:\n",
    "    key = (e.get(\"input\", \"\").strip(), e.get(\"output\", \"\").strip())\n",
    "    if key not in seen and e.get(\"output\", \"\").strip() and \"label\" not in e:\n",
    "        seen.add(key)\n",
    "        examples.append({\"input\": e[\"input\"], \"target\": e[\"output\"]})\n",
    "\n",
    "print(f\"[QG] Unique examples: {len(examples)}\")\n",
    "\n",
    "\n",
    "def make_prompt(entry):\n",
    "    return (\n",
    "        \"Generate exactly ONE concise interview question from the CV snippet. \"\n",
    "        \"Keep it natural (9–20 words) and end with '?'.\\n\"\n",
    "        f\"Snippet: {entry['input']}\\n\"\n",
    "        \"Question:\"\n",
    "    )\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_text\": [make_prompt(e) for e in examples],\n",
    "    \"target_text\": [e[\"target\"] for e in examples]\n",
    "})\n",
    "\n",
    "# ---------------------------\n",
    "# Tokenizer + tokenize\n",
    "# ---------------------------\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def tok_fn(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"input_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=192\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"target_text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=40\n",
    "        )\n",
    "    # Replace pad in labels with -100 for CE loss masking\n",
    "    labels_ids = []\n",
    "    for ids in labels[\"input_ids\"]:\n",
    "        labels_ids.append([(-100 if t == tokenizer.pad_token_id else t) for t in ids])\n",
    "    model_inputs[\"labels\"] = labels_ids\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized = dataset.map(tok_fn, batched=True, remove_columns=[\"input_text\", \"target_text\"])\n",
    "\n",
    "# ---------------------------\n",
    "# Split\n",
    "# ---------------------------\n",
    "splits = tokenized.train_test_split(test_size=0.2, seed=42)\n",
    "train_valid = splits[\"train\"].train_test_split(test_size=0.25, seed=42)\n",
    "train_ds = train_valid[\"train\"]\n",
    "val_ds = train_valid[\"test\"]\n",
    "test_ds = splits[\"test\"]\n",
    "print(f\"[QG] train={len(train_ds)} val={len(val_ds)} test={len(test_ds)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Model\n",
    "# ---------------------------\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "# Correct T5 dropout knobs:\n",
    "model.config.dropout_rate = 0.10\n",
    "model.config.attention_dropout_rate = 0.10\n",
    "model.to(device)\n",
    "\n",
    "# ---------------------------\n",
    "# Trainer\n",
    "# ---------------------------\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(  # <-- seq2seq args\n",
    "    output_dir=\"./results_gen\",\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.06,\n",
    "    logging_dir=\"./logs_gen\",\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    predict_with_generate=True,  # <-- now valid\n",
    "    generation_max_length=32,  # <-- add generation params\n",
    "    generation_num_beams=6,\n",
    "    report_to=\"none\",\n",
    "    save_safetensors=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(  # <-- seq2seq trainer\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ---------------------------\n",
    "# Save\n",
    "# ---------------------------\n",
    "model.save_pretrained(\"./question_generation_model\")\n",
    "tokenizer.save_pretrained(\"./question_generation_model\")\n",
    "print(\"[QG] Saved to ./question_generation_model\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluation (BLEU + semantic)\n",
    "# ---------------------------\n",
    "@torch.inference_mode()\n",
    "def evaluate(val_dataset, samples=200):\n",
    "    model.eval()\n",
    "    sem = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    subset = val_dataset.shuffle(seed=42).select(range(min(samples, len(val_dataset))))\n",
    "\n",
    "    total_bleu = 0.0\n",
    "    total_sim = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for ex in subset:\n",
    "        ids = torch.tensor(ex[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "        att = torch.tensor(ex[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "\n",
    "        # Rebuild target text correctly (replace -100 with pad before decode)\n",
    "        label_ids = [(tid if tid != -100 else tokenizer.pad_token_id) for tid in ex[\"labels\"]]\n",
    "        target = tokenizer.decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "        out = model.generate(\n",
    "            input_ids=ids,\n",
    "            attention_mask=att,\n",
    "            num_beams=6,  # stable\n",
    "            max_new_tokens=32,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        gen = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "        ref = [target.split()]\n",
    "        cand = gen.split()\n",
    "        bleu = sentence_bleu(ref, cand) if cand and ref[0] else 0.0\n",
    "        s = util.pytorch_cos_sim(\n",
    "            sem.encode([target], convert_to_tensor=True),\n",
    "            sem.encode([gen], convert_to_tensor=True),\n",
    "        ).item()\n",
    "\n",
    "        if not math.isnan(bleu) and not math.isnan(s):\n",
    "            total_bleu += bleu\n",
    "            total_sim += s\n",
    "            n += 1\n",
    "\n",
    "    if n == 0:\n",
    "        print(\"[QG] No valid eval samples.\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    print(f\"[QG] Avg BLEU: {total_bleu / n:.4f} | Avg Semantic: {total_sim / n:.4f}\")\n",
    "    return total_bleu / n, total_sim / n\n",
    "\n",
    "\n",
    "evaluate(val_ds)\n"
   ],
   "id": "3f8be27cc5fa68ee",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1007)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QG] Using device: mps\n",
      "[QG] Unique examples: 428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/428 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6fe775ac032f4633a53f60d49ebdf5c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QG] train=256 val=86 test=86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/tm/rvfp5h494516qprtzvrwh8r00000gn/T/ipykernel_34140/48904278.py:137: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(  # <-- seq2seq trainer\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='256' max='256' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [256/256 06:21, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QG] Saved to ./question_generation_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QG] Avg BLEU: 0.7824 | Avg Semantic: 0.9692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7824081357018974, 0.9691774110461391)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T06:35:48.506411Z",
     "start_time": "2025-08-26T06:28:30.642720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train_answer_evaluation_hardneg.py\n",
    "import json, numpy as np, torch\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                      else \"mps\" if torch.backends.mps.is_available()\n",
    "                      else \"cpu\")\n",
    "print(f\"[CLS] device={device}\")\n",
    "\n",
    "# ----- load augmented -----\n",
    "with open(\"converted_dataset_hardneg.json\",\"r\") as f:\n",
    "    rows = json.load(f)\n",
    "\n",
    "texts  = [r[\"input\"] for r in rows]\n",
    "labels = [1 if r[\"label\"] == \"correct\" else 0 for r in rows]\n",
    "ds_all = Dataset.from_dict({\"text\": texts, \"labels\": labels})\n",
    "\n",
    "splits = ds_all.train_test_split(test_size=0.2, seed=42)\n",
    "train_valid = splits[\"train\"].train_test_split(test_size=0.25, seed=42)\n",
    "train_ds, val_ds = train_valid[\"train\"], train_valid[\"test\"]\n",
    "test_ds = splits[\"test\"]\n",
    "print(f\"train={len(train_ds)} val={len(val_ds)} test={len(test_ds)}\")\n",
    "\n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tok_fn(ex):\n",
    "    return tok(ex[\"text\"], padding=False, truncation=True, max_length=512)\n",
    "\n",
    "train_ds = train_ds.map(tok_fn, batched=True)\n",
    "val_ds   = val_ds.map(tok_fn, batched=True)\n",
    "test_ds  = test_ds.map(tok_fn, batched=True)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "y = np.array(train_ds[\"labels\"])\n",
    "w = compute_class_weight(class_weight=\"balanced\", classes=np.array([0,1]), y=y)\n",
    "class_weights = torch.tensor(w, dtype=torch.float).to(device)\n",
    "print(\"[CLS] class weights:\", w)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.config.id2label = {0:\"incorrect\",1:\"correct\"}\n",
    "model.config.label2id = {\"incorrect\":0,\"correct\":1}\n",
    "model.to(device)\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits  = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def metrics_fn(p):\n",
    "    preds = p.predictions.argmax(axis=1)\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average=\"binary\", zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results_cls_hardneg\",\n",
    "    num_train_epochs=6,                    # start a bit lower; dataset is harder\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.06,\n",
    "    logging_dir=\"./logs_cls_hardneg\",\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    "    save_safetensors=True\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=metrics_fn\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save for the API\n",
    "model.save_pretrained(\"./answer_evaluation_model\")\n",
    "tok.save_pretrained(\"./answer_evaluation_model\")\n",
    "print(\"[CLS] Saved to ./answer_evaluation_model\")\n",
    "\n",
    "# Final test set report\n",
    "print(\"[CLS] Test:\", trainer.evaluate(test_ds))\n"
   ],
   "id": "4407f1c228885e2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] device=mps\n",
      "train=1512 val=504 test=504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1512 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dbbf91759194074ba86b5c3e4dda624"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/504 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72c79f7940af453098068295046381e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/504 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e152f6373df24b0aa88348fa1e7b8e61"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] class weights: [0.74556213 1.51807229]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/dasunsathsara/VVH/FinalYearProject-SmartHire-Backend/.venv/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/tm/rvfp5h494516qprtzvrwh8r00000gn/T/ipykernel_34140/4216645387.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [570/570 06:48, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>0.369640</td>\n",
       "      <td>0.884921</td>\n",
       "      <td>0.806630</td>\n",
       "      <td>0.863905</td>\n",
       "      <td>0.834286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Saved to ./answer_evaluation_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:09]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Test: {'eval_loss': 0.3476049304008484, 'eval_accuracy': 0.8809523809523809, 'eval_precision': 0.8021390374331551, 'eval_recall': 0.8670520231213873, 'eval_f1': 0.8333333333333334, 'eval_runtime': 9.599, 'eval_samples_per_second': 52.506, 'eval_steps_per_second': 3.334, 'epoch': 6.0}\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
